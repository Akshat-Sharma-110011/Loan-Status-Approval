{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-06T06:40:13.872840Z",
     "start_time": "2025-05-06T06:40:13.854517Z"
    }
   },
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import logging\n",
    "from src.data.demo_data_preprocessing import (\n",
    "    TargetExtractor,\n",
    "    NaNDuplicatesRemover,\n",
    "    OutlierRemover,\n",
    "    FeatureTransformer,\n",
    "    ColumnAligner,\n",
    ")\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "def preprocess_demo_data(demo_csv_path, output_path=\"artifacts/demo_processed.csv\"):\n",
    "    # Load artifacts\n",
    "    pipeline = joblib.load(\"artifacts/preprocessing_pipeline.pkl\")\n",
    "    transformers = joblib.load(\"artifacts/transformers.pkl\")\n",
    "    outlier_thresholds = joblib.load(\"artifacts/outlier_thresholds.pkl\")\n",
    "    reference_columns = joblib.load(\"artifacts/feature_columns.pkl\")\n",
    "\n",
    "    # Load demo data\n",
    "    demo_df = pd.read_csv(demo_csv_path)\n",
    "    logging.info(f\"Loaded demo data with shape: {demo_df.shape}\")\n",
    "\n",
    "    # Manually rebuild relevant pipeline steps for transform only\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from src.data.demo_data_preprocessing import (\n",
    "        NaNDuplicatesRemover,\n",
    "        OutlierRemover,\n",
    "        FeatureTransformer,\n",
    "        ColumnAligner,\n",
    "        TargetExtractor,\n",
    "    )\n",
    "\n",
    "    # Recreate minimal pipeline for inference (exclude target extraction)\n",
    "    inference_pipeline = Pipeline(steps=[\n",
    "        ('cleaning', NaNDuplicatesRemover()),\n",
    "        ('outlier_removal', OutlierRemover()),\n",
    "        ('feature_transform', FeatureTransformer()),\n",
    "        ('column_alignment', ColumnAligner(reference_columns=reference_columns)),\n",
    "    ])\n",
    "\n",
    "    # Inject fitted parameters\n",
    "    inference_pipeline.named_steps['outlier_removal'].thresholds_ = outlier_thresholds\n",
    "    inference_pipeline.named_steps['feature_transform'].transformers_ = transformers\n",
    "\n",
    "    # Transform demo data\n",
    "    processed_demo = inference_pipeline.transform(demo_df)\n",
    "    logging.info(f\"Processed demo data shape: {processed_demo.shape}\")\n",
    "\n",
    "    # Save output\n",
    "    os.makedirs(\"artifacts\", exist_ok=True)\n",
    "    processed_demo.to_csv(output_path, index=False)\n",
    "    logging.info(f\"Processed demo data saved to: {output_path}\")"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T06:40:20.275852Z",
     "start_time": "2025-05-06T06:40:20.259680Z"
    }
   },
   "cell_type": "code",
   "source": "df.head()",
   "id": "148e90c529cb1753",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   person_age person_gender person_education  person_income  person_emp_exp  \\\n",
       "0        22.0        female           Master        71948.0               0   \n",
       "1        21.0        female      High School        12282.0               0   \n",
       "2        25.0        female      High School        12438.0               3   \n",
       "3        23.0        female         Bachelor        79753.0               0   \n",
       "4        24.0          male           Master        66135.0               1   \n",
       "\n",
       "  person_home_ownership  loan_amnt loan_intent  loan_int_rate  \\\n",
       "0                  RENT    35000.0    PERSONAL          16.02   \n",
       "1                   OWN     1000.0   EDUCATION          11.14   \n",
       "2              MORTGAGE     5500.0     MEDICAL          12.87   \n",
       "3                  RENT    35000.0     MEDICAL          15.23   \n",
       "4                  RENT    35000.0     MEDICAL          14.27   \n",
       "\n",
       "   loan_percent_income  cb_person_cred_hist_length  credit_score  \\\n",
       "0                 0.49                         3.0           561   \n",
       "1                 0.08                         2.0           504   \n",
       "2                 0.44                         3.0           635   \n",
       "3                 0.44                         2.0           675   \n",
       "4                 0.53                         4.0           586   \n",
       "\n",
       "  previous_loan_defaults_on_file  loan_status  \n",
       "0                             No            1  \n",
       "1                            Yes            0  \n",
       "2                             No            1  \n",
       "3                             No            1  \n",
       "4                             No            1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_age</th>\n",
       "      <th>person_gender</th>\n",
       "      <th>person_education</th>\n",
       "      <th>person_income</th>\n",
       "      <th>person_emp_exp</th>\n",
       "      <th>person_home_ownership</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>loan_intent</th>\n",
       "      <th>loan_int_rate</th>\n",
       "      <th>loan_percent_income</th>\n",
       "      <th>cb_person_cred_hist_length</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>previous_loan_defaults_on_file</th>\n",
       "      <th>loan_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.0</td>\n",
       "      <td>female</td>\n",
       "      <td>Master</td>\n",
       "      <td>71948.0</td>\n",
       "      <td>0</td>\n",
       "      <td>RENT</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>PERSONAL</td>\n",
       "      <td>16.02</td>\n",
       "      <td>0.49</td>\n",
       "      <td>3.0</td>\n",
       "      <td>561</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.0</td>\n",
       "      <td>female</td>\n",
       "      <td>High School</td>\n",
       "      <td>12282.0</td>\n",
       "      <td>0</td>\n",
       "      <td>OWN</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>EDUCATION</td>\n",
       "      <td>11.14</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>504</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25.0</td>\n",
       "      <td>female</td>\n",
       "      <td>High School</td>\n",
       "      <td>12438.0</td>\n",
       "      <td>3</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>5500.0</td>\n",
       "      <td>MEDICAL</td>\n",
       "      <td>12.87</td>\n",
       "      <td>0.44</td>\n",
       "      <td>3.0</td>\n",
       "      <td>635</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.0</td>\n",
       "      <td>female</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>79753.0</td>\n",
       "      <td>0</td>\n",
       "      <td>RENT</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>MEDICAL</td>\n",
       "      <td>15.23</td>\n",
       "      <td>0.44</td>\n",
       "      <td>2.0</td>\n",
       "      <td>675</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.0</td>\n",
       "      <td>male</td>\n",
       "      <td>Master</td>\n",
       "      <td>66135.0</td>\n",
       "      <td>1</td>\n",
       "      <td>RENT</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>MEDICAL</td>\n",
       "      <td>14.27</td>\n",
       "      <td>0.53</td>\n",
       "      <td>4.0</td>\n",
       "      <td>586</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T06:41:05.423343Z",
     "start_time": "2025-05-06T06:41:05.412169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('loan_status', axis=1)\n",
    "y = df['loan_status']"
   ],
   "id": "5fbd9adad32930d4",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T06:41:07.378976Z",
     "start_time": "2025-05-06T06:41:06.802403Z"
    }
   },
   "cell_type": "code",
   "source": "preprocess_demo_data('./notebooks/loan_data.csv', './demo_artifacts/demo_processed.csv')",
   "id": "942883fbbff4d98b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 12:11:06,875 - INFO - Loaded demo data with shape: (45000, 14)\n",
      "2025-05-06 12:11:06,875 - INFO - Initial shape before NaN/Duplicate removal: (45000, 14)\n",
      "2025-05-06 12:11:06,918 - INFO - Shape after NaN/Duplicate removal: (45000, 14)\n",
      "2025-05-06 12:11:06,918 - INFO - Initial shape before outlier removal: (45000, 14)\n",
      "2025-05-06 12:11:06,950 - INFO - Shape after outlier removal: (37515, 14)\n",
      "2025-05-06 12:11:06,965 - INFO - Processed demo data shape: (37515, 13)\n",
      "2025-05-06 12:11:07,361 - INFO - Processed demo data saved to: ./demo_artifacts/demo_processed.csv\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T07:21:06.403638Z",
     "start_time": "2025-05-06T07:21:06.112163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import yaml\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Import data preprocessing modules\n",
    "from src.data.data_transformation import (\n",
    "    load_feature_config,\n",
    "    create_preprocessing_pipeline\n",
    ")\n",
    "\n",
    "# Import feature engineering modules\n",
    "from src.features.feature_engineering import (\n",
    "    ColumnStandardizer,\n",
    "    OneHotEncoder,\n",
    "    TargetSeparator,\n",
    "    SMOTEBalancer,\n",
    "    FeatureColumnSaver,\n",
    "    create_feature_engineering_pipeline,\n",
    "    load_feature_store_params\n",
    ")\n",
    "\n",
    "# Import preprocessing components\n",
    "from src.data.demo_data_preprocessing import (\n",
    "    NaNDuplicatesRemover,\n",
    "    OutlierRemover,\n",
    "    FeatureTransformer,\n",
    "    ColumnAligner\n",
    ")\n",
    "\n",
    "# Import utility functions\n",
    "from src.logger import logging, section\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "def preprocess_demo_data(demo_csv_path, output_path=\"artifacts/demo_processed.csv\"):\n",
    "    \"\"\"Preprocess demo data using the saved preprocessing pipeline\"\"\"\n",
    "    section(\"PREPROCESSING DEMO DATA\", level=logging.INFO)\n",
    "\n",
    "    # Load artifacts\n",
    "    pipeline = joblib.load(\"artifacts/preprocessing_pipeline.pkl\")\n",
    "    transformers = joblib.load(\"artifacts/transformers.pkl\")\n",
    "    outlier_thresholds = joblib.load(\"artifacts/outlier_thresholds.pkl\")\n",
    "    reference_columns = joblib.load(\"artifacts/feature_columns.pkl\")\n",
    "\n",
    "    # Load demo data\n",
    "    demo_df = pd.read_csv(demo_csv_path)\n",
    "    logging.info(f\"Loaded demo data with shape: {demo_df.shape}\")\n",
    "\n",
    "    # Recreate minimal pipeline for inference\n",
    "    cleaning = NaNDuplicatesRemover()\n",
    "    outlier_removal = OutlierRemover()\n",
    "    outlier_removal.thresholds_ = outlier_thresholds\n",
    "    feature_transform = FeatureTransformer()\n",
    "    feature_transform.transformers_ = transformers\n",
    "    column_alignment = ColumnAligner(reference_columns=reference_columns)\n",
    "\n",
    "    # Sequentially apply transformations with logging\n",
    "    data = demo_df.copy()\n",
    "\n",
    "    logging.info(f\"Step 1: Raw input shape: {data.shape}\")\n",
    "    data = cleaning.fit_transform(data)\n",
    "    logging.info(f\"Step 2: After NaN/Duplicates Removal: {data.shape}\")\n",
    "\n",
    "    data = outlier_removal.transform(data)\n",
    "    logging.info(f\"Step 3: After Outlier Removal: {data.shape}\")\n",
    "\n",
    "    data = feature_transform.transform(data)\n",
    "    logging.info(f\"Step 4: After Feature Transformation: {data.shape}\")\n",
    "\n",
    "    data = column_alignment.transform(data)\n",
    "    logging.info(f\"Step 5: After Column Alignment: {data.shape}\")\n",
    "\n",
    "    # Save output\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    data.to_csv(output_path, index=False)\n",
    "    logging.info(f\"Processed demo data saved to: {output_path}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "def apply_feature_engineering(processed_data, config_path, output_path=\"artifacts/demo_features.csv\"):\n",
    "    \"\"\"Apply feature engineering to preprocessed data\"\"\"\n",
    "    section(\"FEATURE ENGINEERING PIPELINE\", level=logging.INFO, char='*', length=80)\n",
    "\n",
    "    try:\n",
    "        # Load feature configuration\n",
    "        feature_config = load_feature_store_params(config_path)\n",
    "\n",
    "        # Get target column (use default if not specified)\n",
    "        target_col = None\n",
    "        if 'target_cols' in feature_config and feature_config['target_cols']:\n",
    "            target_col = feature_config['target_cols'][0].lower().strip()\n",
    "        else:\n",
    "            target_col = 'loan_status'  # Default target column\n",
    "            logging.warning(f\"No target column specified in config. Using default: {target_col}\")\n",
    "\n",
    "        # Create feature engineering pipeline for inference\n",
    "        feat_pipeline, _ = create_feature_engineering_pipeline(\n",
    "            feature_config,\n",
    "            target_col=target_col,\n",
    "            mode='test'  # Use test mode for demo data (inference)\n",
    "        )\n",
    "\n",
    "        # Apply feature engineering\n",
    "        logging.info(\"Applying feature engineering to demo data\")\n",
    "        features = feat_pipeline.fit_transform(processed_data)\n",
    "        logging.info(f\"Feature engineering completed. Output shape: {features.shape}\")\n",
    "\n",
    "        # Save engineered features\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        features.to_csv(output_path, index=False)\n",
    "        logging.info(f\"Feature engineered data saved to: {output_path}\")\n",
    "\n",
    "        # Save the feature engineering pipeline\n",
    "        pipeline_path = os.path.join(os.path.dirname(output_path), \"feature_engineering_pipeline.joblib\")\n",
    "        joblib.dump(feat_pipeline, pipeline_path)\n",
    "        logging.info(f\"Feature engineering pipeline saved to: {pipeline_path}\")\n",
    "\n",
    "        # Return the engineered features\n",
    "        return features\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Feature engineering pipeline failed: {str(e)}\")\n",
    "        section(\"FEATURE ENGINEERING PIPELINE FAILED\", level=logging.ERROR, char='!', length=80)\n",
    "        raise\n",
    "\n",
    "def demo_complete_pipeline(demo_csv_path, config_path=\"references/feature_store.yaml\"):\n",
    "    \"\"\"Demo the complete pipeline: preprocessing + feature engineering\"\"\"\n",
    "    section(\"COMPLETE PIPELINE DEMONSTRATION\", level=logging.INFO, char='#', length=80)\n",
    "\n",
    "    try:\n",
    "        # Step 1: Preprocess the demo data\n",
    "        logging.info(\"Step 1: Preprocessing demo data\")\n",
    "        processed_data = preprocess_demo_data(demo_csv_path)\n",
    "\n",
    "        # Step 2: Apply feature engineering\n",
    "        logging.info(\"Step 2: Applying feature engineering\")\n",
    "        features = apply_feature_engineering(processed_data, config_path)\n",
    "\n",
    "        section(\"PIPELINE DEMONSTRATION COMPLETED SUCCESSFULLY\", level=logging.INFO, char='#', length=80)\n",
    "        return features\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Complete pipeline demonstration failed: {str(e)}\")\n",
    "        section(\"PIPELINE DEMONSTRATION FAILED\", level=logging.ERROR, char='!', length=80)\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to test the pipeline\"\"\"\n",
    "    try:\n",
    "        # Set up directories\n",
    "        os.makedirs(\"artifacts\", exist_ok=True)\n",
    "        os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "        # Path to demo data\n",
    "        demo_data_path = \"data/external/demo.csv\"\n",
    "        config_path = \"references/feature_store.yaml\"\n",
    "\n",
    "        # Check if files exist before running the pipeline\n",
    "        if not os.path.exists(demo_data_path):\n",
    "            logging.error(f\"Demo data not found at {demo_data_path}. Please provide demo data.\")\n",
    "            return\n",
    "\n",
    "        if not os.path.exists(config_path):\n",
    "            logging.error(f\"Config not found at {config_path}. Please provide configuration.\")\n",
    "            return\n",
    "\n",
    "        # Run the complete pipeline\n",
    "        demo_complete_pipeline(demo_data_path, config_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Main function failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "839954c37e74f9dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | \n",
      "################################################################################\n",
      "                        COMPLETE PIPELINE DEMONSTRATION                         \n",
      "################################################################################\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Step 1: Preprocessing demo data\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | \n",
      "==================================================\n",
      "             PREPROCESSING DEMO DATA              \n",
      "==================================================\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Loaded demo data with shape: (4, 13)\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Step 1: Raw input shape: (4, 13)\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Initial shape before NaN/Duplicate removal: (4, 13)\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Shape after NaN/Duplicate removal: (4, 13)\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Step 2: After NaN/Duplicates Removal: (4, 13)\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Initial shape before outlier removal: (4, 13)\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Shape after outlier removal: (3, 13)\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Step 3: After Outlier Removal: (3, 13)\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Step 4: After Feature Transformation: (3, 13)\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Step 5: After Column Alignment: (3, 13)\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Processed demo data saved to: artifacts/demo_processed.csv\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Step 2: Applying feature engineering\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | \n",
      "********************************************************************************\n",
      "                          FEATURE ENGINEERING PIPELINE                          \n",
      "********************************************************************************\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | \n",
      "==================================================\n",
      "       Loading Feature Store Configuration        \n",
      "==================================================\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Feature config loaded from references/feature_store.yaml\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Applying feature engineering to demo data\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | \n",
      "==================================================\n",
      "            STANDARDIZING COLUMN NAMES            \n",
      "==================================================\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Standardizing column names to lowercase\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Column names standardized\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | \n",
      "==================================================\n",
      "            ONE-HOT ENCODING - FITTING            \n",
      "==================================================\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Categorical columns from config: ['person_gender', 'person_education', 'person_home_ownership', 'loan_intent', 'previous_loan_defaults_on_file']\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Proceeding with encoding for available columns: ['person_gender', 'person_education', 'person_home_ownership', 'loan_intent', 'previous_loan_defaults_on_file']\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Column 'person_gender' before standardization - unique values: 1\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Column 'person_gender' after standardization - unique values: 1\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Column 'person_education' before standardization - unique values: 2\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Column 'person_education' after standardization - unique values: 2\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Column 'person_home_ownership' before standardization - unique values: 2\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Column 'person_home_ownership' after standardization - unique values: 2\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Column 'loan_intent' before standardization - unique values: 2\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Column 'loan_intent' after standardization - unique values: 2\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Column 'previous_loan_defaults_on_file' before standardization - unique values: 2\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Column 'previous_loan_defaults_on_file' after standardization - unique values: 2\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Applied one-hot encoding to: ['person_gender', 'person_education', 'person_home_ownership', 'loan_intent', 'previous_loan_defaults_on_file']\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Shape change: (3, 13) -> (3, 12)\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | New columns added: -1\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Stored 12 encoded columns during fit\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | \n",
      "==================================================\n",
      "           ONE-HOT ENCODING - TRANSFORM           \n",
      "==================================================\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Applying encoding to columns: ['person_gender', 'person_education', 'person_home_ownership', 'loan_intent', 'previous_loan_defaults_on_file']\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Column 'person_gender' before standardization - unique values: 1\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Column 'person_gender' after standardization - unique values: 1\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Column 'person_education' before standardization - unique values: 2\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Column 'person_education' after standardization - unique values: 2\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Column 'person_home_ownership' before standardization - unique values: 2\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Column 'person_home_ownership' after standardization - unique values: 2\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Column 'loan_intent' before standardization - unique values: 2\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Column 'loan_intent' after standardization - unique values: 2\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Column 'previous_loan_defaults_on_file' before standardization - unique values: 2\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Column 'previous_loan_defaults_on_file' after standardization - unique values: 2\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Applied one-hot encoding to: ['person_gender', 'person_education', 'person_home_ownership', 'loan_intent', 'previous_loan_defaults_on_file']\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Shape change: (3, 13) -> (3, 12)\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | New columns added: -1\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Feature engineering completed. Output shape: (3, 12)\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Feature engineered data saved to: artifacts/demo_features.csv\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | Feature engineering pipeline saved to: artifacts\\feature_engineering_pipeline.joblib\n",
      "[✓] 2025-05-06 12:51:06 | root            | INFO  | \n",
      "################################################################################\n",
      "                 PIPELINE DEMONSTRATION COMPLETED SUCCESSFULLY                  \n",
      "################################################################################\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T08:34:33.198684Z",
     "start_time": "2025-05-06T08:34:33.063744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Option 1: If the module successfully loads the pipeline\n",
    "import pandas as pd\n",
    "from src.features.demo_engineering import drop_target_column\n",
    "\n",
    "# Load the pipeline directly (more reliable)\n",
    "import joblib\n",
    "pipeline = joblib.load('./models/pipe.pkl')\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(\"data/external/demo.csv\")\n",
    "\n",
    "# Use the pipeline\n",
    "processed_data = pipeline.transform(df)\n",
    "\n",
    "# Option 2: Define everything locally (most reliable)\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Define the same function with the EXACT same name and implementation\n",
    "def drop_target_column(X):\n",
    "    \"\"\"Drop the target column from the DataFrame.\"\"\"\n",
    "    return X.drop(columns=['loan_status'], errors='ignore')\n",
    "\n",
    "# Now load the pipeline\n",
    "pipeline = joblib.load('./models/pipe.pkl')\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(\"data/external/demo.csv\")\n",
    "\n",
    "# Use the pipeline\n",
    "processed_data = pipeline.transform(df)\n",
    "\n",
    "# Option 3: If you want to fix your original code\n",
    "# Make sure you're importing the right function and loading the pipeline yourself\n",
    "from src.features.demo_engineering import drop_target_column\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(\"data/external/demo.csv\")\n",
    "\n",
    "# Load the pipeline explicitly\n",
    "import joblib\n",
    "pipeline = joblib.load('./models/pipe.pkl')\n",
    "\n",
    "# Now transform\n",
    "processed_data = pipeline.transform(df)"
   ],
   "id": "2e66f1215ec46433",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] 2025-05-06 14:04:33 | root            | INFO  | \n",
      "==================================================\n",
      "                  DATA CLEANING                   \n",
      "==================================================\n",
      "[✓] 2025-05-06 14:04:33 | root            | INFO  | No NaN values found in the dataset\n",
      "[✓] 2025-05-06 14:04:33 | root            | INFO  | No duplicate rows found in the dataset\n",
      "[✓] 2025-05-06 14:04:33 | root            | INFO  | After cleaning: (3, 13)\n",
      "[✓] 2025-05-06 14:04:33 | root            | INFO  | \n",
      "==================================================\n",
      "           OUTLIER REMOVAL - TRANSFORM            \n",
      "==================================================\n",
      "[✓] 2025-05-06 14:04:33 | root            | INFO  | Processing outliers for column: person_age\n",
      "[✗] 2025-05-06 14:04:33 | root            | ERROR | Error processing outliers for column 'person_age': ufunc 'less_equal' did not contain a loop with signature matching types (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.StrDType'>) -> None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_UFuncNoLoopError.__init__() missing 1 required positional argument: 'dtypes'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUFuncTypeError\u001B[0m                            Traceback (most recent call last)",
      "File \u001B[1;32mD:\\MLOps-Projects\\Loan-Status-Approval\\src\\data\\data_preprocessing.py:134\u001B[0m, in \u001B[0;36mOutlierRemover.transform\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    133\u001B[0m before \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m--> 134\u001B[0m X \u001B[38;5;241m=\u001B[39m X[(\u001B[43mX\u001B[49m\u001B[43m[\u001B[49m\u001B[43mcol\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m>\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mlower\u001B[49m) \u001B[38;5;241m&\u001B[39m (X[col] \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m upper)]\n\u001B[0;32m    135\u001B[0m removed \u001B[38;5;241m=\u001B[39m before \u001B[38;5;241m-\u001B[39m X\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Loan-Status-Approval\\lib\\site-packages\\pandas\\core\\ops\\common.py:76\u001B[0m, in \u001B[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001B[1;34m(self, other)\u001B[0m\n\u001B[0;32m     74\u001B[0m other \u001B[38;5;241m=\u001B[39m item_from_zerodim(other)\n\u001B[1;32m---> 76\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmethod\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mother\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Loan-Status-Approval\\lib\\site-packages\\pandas\\core\\arraylike.py:60\u001B[0m, in \u001B[0;36mOpsMixin.__ge__\u001B[1;34m(self, other)\u001B[0m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;129m@unpack_zerodim_and_defer\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__ge__\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__ge__\u001B[39m(\u001B[38;5;28mself\u001B[39m, other):\n\u001B[1;32m---> 60\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cmp_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moperator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mge\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Loan-Status-Approval\\lib\\site-packages\\pandas\\core\\series.py:6119\u001B[0m, in \u001B[0;36mSeries._cmp_method\u001B[1;34m(self, other, op)\u001B[0m\n\u001B[0;32m   6117\u001B[0m rvalues \u001B[38;5;241m=\u001B[39m extract_array(other, extract_numpy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, extract_range\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m-> 6119\u001B[0m res_values \u001B[38;5;241m=\u001B[39m \u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcomparison_op\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   6121\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_construct_result(res_values, name\u001B[38;5;241m=\u001B[39mres_name)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Loan-Status-Approval\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:344\u001B[0m, in \u001B[0;36mcomparison_op\u001B[1;34m(left, right, op)\u001B[0m\n\u001B[0;32m    343\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m lvalues\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mobject\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(rvalues, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m--> 344\u001B[0m     res_values \u001B[38;5;241m=\u001B[39m \u001B[43mcomp_method_OBJECT_ARRAY\u001B[49m\u001B[43m(\u001B[49m\u001B[43mop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrvalues\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    346\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Loan-Status-Approval\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:129\u001B[0m, in \u001B[0;36mcomp_method_OBJECT_ARRAY\u001B[1;34m(op, x, y)\u001B[0m\n\u001B[0;32m    128\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 129\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mlibops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscalar_compare\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mravel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    130\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\u001B[38;5;241m.\u001B[39mreshape(x\u001B[38;5;241m.\u001B[39mshape)\n",
      "File \u001B[1;32mops.pyx:107\u001B[0m, in \u001B[0;36mpandas._libs.ops.scalar_compare\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mUFuncTypeError\u001B[0m: ufunc 'less_equal' did not contain a loop with signature matching types (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.StrDType'>) -> None",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[42], line 13\u001B[0m\n\u001B[0;32m     10\u001B[0m df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata/external/demo.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Use the pipeline\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m processed_data \u001B[38;5;241m=\u001B[39m \u001B[43mpipeline\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# Option 2: Define everything locally (most reliable)\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpd\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Loan-Status-Approval\\lib\\site-packages\\sklearn\\pipeline.py:903\u001B[0m, in \u001B[0;36mPipeline.transform\u001B[1;34m(self, X, **params)\u001B[0m\n\u001B[0;32m    901\u001B[0m Xt \u001B[38;5;241m=\u001B[39m X\n\u001B[0;32m    902\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _, name, transform \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iter():\n\u001B[1;32m--> 903\u001B[0m     Xt \u001B[38;5;241m=\u001B[39m transform\u001B[38;5;241m.\u001B[39mtransform(Xt, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mrouted_params[name]\u001B[38;5;241m.\u001B[39mtransform)\n\u001B[0;32m    904\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Xt\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Loan-Status-Approval\\lib\\site-packages\\sklearn\\pipeline.py:903\u001B[0m, in \u001B[0;36mPipeline.transform\u001B[1;34m(self, X, **params)\u001B[0m\n\u001B[0;32m    901\u001B[0m Xt \u001B[38;5;241m=\u001B[39m X\n\u001B[0;32m    902\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _, name, transform \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iter():\n\u001B[1;32m--> 903\u001B[0m     Xt \u001B[38;5;241m=\u001B[39m transform\u001B[38;5;241m.\u001B[39mtransform(Xt, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mrouted_params[name]\u001B[38;5;241m.\u001B[39mtransform)\n\u001B[0;32m    904\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Xt\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Loan-Status-Approval\\lib\\site-packages\\sklearn\\utils\\_set_output.py:313\u001B[0m, in \u001B[0;36m_wrap_method_output.<locals>.wrapped\u001B[1;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[0;32m    311\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[0;32m    312\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 313\u001B[0m     data_to_wrap \u001B[38;5;241m=\u001B[39m f(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    314\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m    315\u001B[0m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[0;32m    316\u001B[0m         return_tuple \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    317\u001B[0m             _wrap_data_with_container(method, data_to_wrap[\u001B[38;5;241m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[0;32m    318\u001B[0m             \u001B[38;5;241m*\u001B[39mdata_to_wrap[\u001B[38;5;241m1\u001B[39m:],\n\u001B[0;32m    319\u001B[0m         )\n",
      "File \u001B[1;32mD:\\MLOps-Projects\\Loan-Status-Approval\\src\\data\\data_preprocessing.py:144\u001B[0m, in \u001B[0;36mOutlierRemover.transform\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    142\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    143\u001B[0m         logging\u001B[38;5;241m.\u001B[39merror(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError processing outliers for column \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcol\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(e)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 144\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28;43mtype\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43me\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mcol\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m: \u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43me\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    146\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m total_removed \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    147\u001B[0m     removal_percentage \u001B[38;5;241m=\u001B[39m (total_removed \u001B[38;5;241m/\u001B[39m initial_size) \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m100\u001B[39m\n",
      "\u001B[1;31mTypeError\u001B[0m: _UFuncNoLoopError.__init__() missing 1 required positional argument: 'dtypes'"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T09:19:02.957253Z",
     "start_time": "2025-05-06T09:18:41.446850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import logging\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, precision_recall_curve, auc\n",
    "from catboost import CatBoostClassifier\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='[✓] %(asctime)s | %(name)-15s | %-5s | %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Custom transformer for IQR-based outlier removal with detailed logging\n",
    "class IQROutlierRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols=None, factor=1.5):\n",
    "        self.cols = cols\n",
    "        self.factor = factor\n",
    "        self.Q1 = {}\n",
    "        self.Q3 = {}\n",
    "        logger.info(f\"Initialized IQROutlierRemover with factor={factor}\")\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        logger.info(f\"Fitting IQROutlierRemover on data with shape {X.shape}\")\n",
    "        logger.info(f\"Input columns: {list(X.columns)}\")\n",
    "\n",
    "        X_ = X.copy()\n",
    "        if self.cols is None:\n",
    "            self.cols = X_.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "            logger.info(f\"Auto-detected numeric columns: {self.cols}\")\n",
    "        else:\n",
    "            logger.info(f\"Using specified columns: {self.cols}\")\n",
    "\n",
    "        for col in self.cols:\n",
    "            self.Q1[col] = X_[col].quantile(0.25)\n",
    "            self.Q3[col] = X_[col].quantile(0.75)\n",
    "            logger.info(f\"Column '{col}': Q1={self.Q1[col]:.4f}, Q3={self.Q3[col]:.4f}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        logger.info(f\"Transforming data with IQROutlierRemover, input shape: {X.shape}\")\n",
    "        X_ = X.copy()\n",
    "\n",
    "        outlier_counts = {}\n",
    "        for col in self.cols:\n",
    "            IQR = self.Q3[col] - self.Q1[col]\n",
    "            lower_bound = self.Q1[col] - (self.factor * IQR)\n",
    "            upper_bound = self.Q3[col] + (self.factor * IQR)\n",
    "\n",
    "            logger.info(f\"Column '{col}': IQR={IQR:.4f}, lower_bound={lower_bound:.4f}, upper_bound={upper_bound:.4f}\")\n",
    "\n",
    "            # Count outliers before replacing\n",
    "            lower_outliers = (X_[col] < lower_bound).sum()\n",
    "            upper_outliers = (X_[col] > upper_bound).sum()\n",
    "            outlier_counts[col] = {'lower': lower_outliers, 'upper': upper_outliers, 'total': lower_outliers + upper_outliers}\n",
    "\n",
    "            # Replace outliers with NaN\n",
    "            X_[col] = np.where((X_[col] < lower_bound) | (X_[col] > upper_bound), np.nan, X_[col])\n",
    "\n",
    "            # Log count of NaN values\n",
    "            nan_count = X_[col].isna().sum()\n",
    "            logger.info(f\"Column '{col}': {lower_outliers} lower outliers, {upper_outliers} upper outliers, {nan_count} NaN values\")\n",
    "\n",
    "            # Fill NaN with median\n",
    "            median_value = X_[col].median()\n",
    "            X_[col] = X_[col].fillna(median_value)\n",
    "            logger.info(f\"Column '{col}': Filled NaN values with median {median_value:.4f}\")\n",
    "\n",
    "        logger.info(f\"IQROutlierRemover transformation complete, output shape: {X_.shape}\")\n",
    "        logger.info(f\"Outlier summary: {outlier_counts}\")\n",
    "\n",
    "        return X_\n",
    "\n",
    "def log_dataframe_info(df, name):\n",
    "    \"\"\"Helper function to log dataframe information\"\"\"\n",
    "    logger.info(f\"{name} shape: {df.shape}\")\n",
    "    logger.info(f\"{name} columns: {list(df.columns)}\")\n",
    "    logger.info(f\"{name} data types:\\n{df.dtypes}\")\n",
    "    logger.info(f\"{name} missing values:\\n{df.isna().sum()}\")\n",
    "    logger.info(f\"{name} summary statistics:\\n{df.describe().transpose()}\")\n",
    "\n",
    "def load_feature_store(file_path='./references/feature_store.yaml'):\n",
    "    \"\"\"\n",
    "    Load feature store YAML file and return column names for numeric and categorical features.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading feature store from {file_path}\")\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            feature_store = yaml.safe_load(f)\n",
    "\n",
    "        numeric_cols = feature_store.get('numerical_cols', [])\n",
    "        categorical_cols = feature_store.get('categorical_cols', [])\n",
    "\n",
    "        logger.info(f\"Loaded numeric columns: {numeric_cols}\")\n",
    "        logger.info(f\"Loaded categorical columns: {categorical_cols}\")\n",
    "\n",
    "        return numeric_cols, categorical_cols\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading feature store: {str(e)}\")\n",
    "        logger.warning(\"Returning empty column lists\")\n",
    "        return [], []\n",
    "\n",
    "class LoggingTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Custom transformer that logs data without modifying it\"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self._feature_names = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            logger.info(f\"{self.name} fit - DataFrame shape: {X.shape}, columns: {list(X.columns)}\")\n",
    "        else:\n",
    "            logger.info(f\"{self.name} fit - array shape: {X.shape}\")\n",
    "            if self._feature_names is not None:\n",
    "                logger.info(f\"{self.name} fit - feature names: {self._feature_names}\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            logger.info(f\"{self.name} transform - DataFrame shape: {X.shape}, columns: {list(X.columns)}\")\n",
    "        else:\n",
    "            logger.info(f\"{self.name} transform - array shape: {X.shape}\")\n",
    "            if self._feature_names is not None:\n",
    "                logger.info(f\"{self.name} transform - feature names: {self._feature_names}\")\n",
    "        return X\n",
    "\n",
    "    def set_feature_names(self, names):\n",
    "        self._feature_names = names\n",
    "        logger.info(f\"{self.name} - Set feature names: {names}\")\n",
    "\n",
    "# New class to capture and log one-hot encoded feature names\n",
    "class OneHotEncoderWithLogging(OneHotEncoder):\n",
    "    \"\"\"OneHotEncoder that logs the feature names it creates\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.feature_names_out_ = None\n",
    "        self.categories_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        result = super().fit(X, y)\n",
    "\n",
    "        # Get and log the categories found\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            column_names = X.columns\n",
    "        else:\n",
    "            column_names = [f\"col_{i}\" for i in range(X.shape[1])]\n",
    "\n",
    "        # Create and store feature names\n",
    "        self.feature_names_out_ = []\n",
    "        for i, (col, cats) in enumerate(zip(column_names, self.categories_)):\n",
    "            # Skip the first category since drop_first=True\n",
    "            for cat in cats[1:]:\n",
    "                self.feature_names_out_.append(f\"{col}_{cat}\")\n",
    "\n",
    "        logger.info(f\"OneHotEncoder created the following features:\")\n",
    "        for feature in self.feature_names_out_:\n",
    "            logger.info(f\"  - {feature}\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        if self.feature_names_out_ is not None:\n",
    "            return np.array(self.feature_names_out_)\n",
    "        return super().get_feature_names_out(input_features)\n",
    "\n",
    "def build_pipeline(X, target_column='loan_status', random_state=42):\n",
    "    \"\"\"\n",
    "    Build a complete sklearn pipeline with outlier removal, power transformation,\n",
    "    one-hot encoding, SMOTE, and CatBoostClassifier, with detailed logging\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting pipeline construction\")\n",
    "    logger.info(f\"Input DataFrame shape: {X.shape}\")\n",
    "    logger.info(f\"Input columns: {list(X.columns)}\")\n",
    "    logger.info(f\"Target column: {target_column}\")\n",
    "\n",
    "    # Load column types from feature_store.yaml\n",
    "    numeric_cols, categorical_cols = load_feature_store()\n",
    "\n",
    "    # Auto-detect column types if not provided or if empty\n",
    "    if not numeric_cols and not categorical_cols:\n",
    "        logger.info(\"No columns found in feature store, auto-detecting column types\")\n",
    "        numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "        categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        logger.info(f\"Auto-detected numeric columns: {numeric_cols}\")\n",
    "        logger.info(f\"Auto-detected categorical columns: {categorical_cols}\")\n",
    "\n",
    "    # Remove target column from feature lists if present\n",
    "    if target_column in numeric_cols:\n",
    "        numeric_cols.remove(target_column)\n",
    "        logger.info(f\"Removed target column '{target_column}' from numeric columns\")\n",
    "\n",
    "    if target_column in categorical_cols:\n",
    "        categorical_cols.remove(target_column)\n",
    "        logger.info(f\"Removed target column '{target_column}' from categorical columns\")\n",
    "\n",
    "    logger.info(f\"Final numeric columns: {numeric_cols}\")\n",
    "    logger.info(f\"Final categorical columns: {categorical_cols}\")\n",
    "\n",
    "    # Define preprocessing steps with logging\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('log_input', LoggingTransformer('Numeric preprocessing - input')),\n",
    "        ('outlier_remover', IQROutlierRemover()),\n",
    "        ('power_transformer', PowerTransformer(method='yeo-johnson')),\n",
    "        ('log_output', LoggingTransformer('Numeric preprocessing - output'))\n",
    "    ])\n",
    "    logger.info(\"Created numeric transformation pipeline\")\n",
    "\n",
    "    # Use our new OneHotEncoderWithLogging instead of the standard OneHotEncoder\n",
    "    # Set drop_first=True to avoid sparse output and prevent dummy variable trap\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('log_input', LoggingTransformer('Categorical preprocessing - input')),\n",
    "        ('onehot', OneHotEncoderWithLogging(handle_unknown='ignore', sparse_output=False, drop='first')),\n",
    "        ('log_output', LoggingTransformer('Categorical preprocessing - output'))\n",
    "    ])\n",
    "    logger.info(\"Created categorical transformation pipeline with drop_first=True\")\n",
    "\n",
    "    # Column transformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "        ],\n",
    "        remainder='drop',  # Drop any columns not specified\n",
    "        verbose_feature_names_out=True  # Ensure verbose feature names that include transformer name\n",
    "    )\n",
    "    logger.info(\"Created column transformer\")\n",
    "\n",
    "    # Configure CatBoost classifier\n",
    "    classifier = CatBoostClassifier(\n",
    "        iterations=800,\n",
    "        learning_rate=0.167,\n",
    "        depth=6,\n",
    "        loss_function='Logloss',\n",
    "        eval_metric='AUC',\n",
    "        random_seed=random_state,\n",
    "        verbose=100,  # Show progress every 100 iterations\n",
    "        thread_count=4\n",
    "    )\n",
    "    logger.info(f\"Configured CatBoostClassifier with parameters: {classifier.get_params()}\")\n",
    "\n",
    "    # Full pipeline\n",
    "    pipeline = ImbPipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('log_before_smote', LoggingTransformer('Before SMOTE')),\n",
    "        ('smote', SMOTE(random_state=random_state)),\n",
    "        ('log_after_smote', LoggingTransformer('After SMOTE')),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    logger.info(\"Final pipeline created\")\n",
    "\n",
    "    # Function to extract and log feature names after pipeline fitting\n",
    "    def extract_feature_names(fitted_pipeline):\n",
    "        \"\"\"Extract and log the feature names from the fitted pipeline\"\"\"\n",
    "        try:\n",
    "            # Get one-hot encoded feature names from the preprocessor\n",
    "            cat_transformer = fitted_pipeline.named_steps['preprocessor'].named_transformers_['cat']\n",
    "            onehotencoder = cat_transformer.named_steps['onehot']\n",
    "\n",
    "            # Log categorical feature mapping\n",
    "            logger.info(\"=== ONE-HOT ENCODED FEATURE NAMES ===\")\n",
    "            if hasattr(onehotencoder, 'feature_names_out_'):\n",
    "                for feature in onehotencoder.feature_names_out_:\n",
    "                    logger.info(f\"  - {feature}\")\n",
    "            else:\n",
    "                # Try to reconstruct the feature names\n",
    "                if hasattr(onehotencoder, 'categories_'):\n",
    "                    categorical_cols_used = fitted_pipeline.named_steps['preprocessor'].transformers_[1][2]\n",
    "\n",
    "                    for i, (col, categories) in enumerate(zip(categorical_cols_used, onehotencoder.categories_)):\n",
    "                        logger.info(f\"Feature '{col}' encoded to:\")\n",
    "                        # Skip the first category as it's dropped\n",
    "                        for cat in categories[1:]:\n",
    "                            feature_name = f\"{col}_{cat}\"\n",
    "                            logger.info(f\"  - {feature_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error extracting one-hot encoded feature names: {str(e)}\")\n",
    "\n",
    "    # Add the extract_feature_names method to the pipeline\n",
    "    pipeline.extract_feature_names = extract_feature_names\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, X_train=None, y_train=None):\n",
    "    \"\"\"\n",
    "    Evaluate the model performance using various classification metrics with detailed logging\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting model evaluation\")\n",
    "    logger.info(f\"Test data shape: X_test={X_test.shape}, y_test={y_test.shape}\")\n",
    "    if X_train is not None:\n",
    "        logger.info(f\"Train data shape: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
    "\n",
    "    # Make predictions\n",
    "    logger.info(\"Generating predictions on test data\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Log class distribution of actual vs predicted\n",
    "    actual_dist = pd.Series(y_test).value_counts()\n",
    "    predicted_dist = pd.Series(y_pred).value_counts()\n",
    "    logger.info(f\"Actual class distribution: {actual_dist.to_dict()}\")\n",
    "    logger.info(f\"Predicted class distribution: {predicted_dist.to_dict()}\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    logger.info(f\"Test accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    class_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    logger.info(f\"Classification report:\\n{pd.DataFrame(class_report).transpose()}\")\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    logger.info(f\"Confusion matrix:\\n{conf_matrix}\")\n",
    "\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    logger.info(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "    # Calculate precision-recall AUC\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    logger.info(f\"PR AUC: {pr_auc:.4f}\")\n",
    "\n",
    "    # Compile results\n",
    "    results = {\n",
    "        'accuracy': accuracy,\n",
    "        'classification_report': class_report,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'roc_auc': roc_auc,\n",
    "        'pr_auc': pr_auc\n",
    "    }\n",
    "\n",
    "    # If training data is provided, check for overfitting\n",
    "    if X_train is not None and y_train is not None:\n",
    "        logger.info(\"Generating predictions on training data to check for overfitting\")\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "        results['train_accuracy'] = train_accuracy\n",
    "        results['overfit_ratio'] = train_accuracy / accuracy if accuracy > 0 else float('inf')\n",
    "\n",
    "        logger.info(f\"Train accuracy: {train_accuracy:.4f}\")\n",
    "        logger.info(f\"Overfit ratio (train/test): {results['overfit_ratio']:.4f}\")\n",
    "        if results['overfit_ratio'] > 1.1:\n",
    "            logger.warning(f\"Potential overfitting detected: train/test accuracy ratio = {results['overfit_ratio']:.4f}\")\n",
    "\n",
    "    logger.info(\"Model evaluation complete\")\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    logger.info(\"=== Starting ML Pipeline Execution ===\")\n",
    "\n",
    "    # Load dataset\n",
    "    logger.info(\"Loading dataset\")\n",
    "    try:\n",
    "        df = pd.read_csv('./notebooks/loan_data.csv')\n",
    "        logger.info(f\"Dataset loaded successfully with shape: {df.shape}\")\n",
    "        log_dataframe_info(df, \"Original dataset\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading dataset: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    # Check for target column\n",
    "    target_column = 'loan_status'\n",
    "    if target_column not in df.columns:\n",
    "        logger.error(f\"Target column '{target_column}' not found in dataset\")\n",
    "        logger.info(f\"Available columns: {list(df.columns)}\")\n",
    "        return\n",
    "\n",
    "    # Log target distribution\n",
    "    target_distribution = df[target_column].value_counts()\n",
    "    logger.info(f\"Target distribution:\\n{target_distribution}\")\n",
    "\n",
    "    # Calculate and format percentage distribution properly\n",
    "    percentage_distribution = 100 * target_distribution / len(df)\n",
    "    logger.info(f\"Target distribution percentage:\\n{percentage_distribution.to_string()}\")\n",
    "\n",
    "    # Split data\n",
    "    logger.info(\"Splitting dataset into train and test sets\")\n",
    "    X = df.drop(target_column, axis=1)\n",
    "    y = df[target_column]\n",
    "    logger.info(f\"X shape after dropping target: {X.shape}\")\n",
    "    logger.info(f\"y shape: {y.shape}\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Train-test split complete: X_train={X_train.shape}, X_test={X_test.shape}, y_train={y_train.shape}, y_test={y_test.shape}\")\n",
    "    logger.info(f\"Train target distribution: {pd.Series(y_train).value_counts().to_dict()}\")\n",
    "    logger.info(f\"Test target distribution: {pd.Series(y_test).value_counts().to_dict()}\")\n",
    "\n",
    "    # Build pipeline\n",
    "    logger.info(\"Building pipeline\")\n",
    "    pipeline = build_pipeline(\n",
    "        X_train,\n",
    "        target_column=target_column,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Fit pipeline\n",
    "    logger.info(\"Fitting pipeline\")\n",
    "    try:\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        logger.info(\"Pipeline fitting complete\")\n",
    "\n",
    "        # Extract and log the one-hot encoded feature names\n",
    "        pipeline.extract_feature_names(pipeline)\n",
    "\n",
    "        # Get feature importances from CatBoost if possible\n",
    "        try:\n",
    "            feature_importances = pipeline.named_steps['classifier'].feature_importances_\n",
    "\n",
    "            # Try to get feature names from preprocessor\n",
    "            try:\n",
    "                cat_transformer = pipeline.named_steps['preprocessor'].named_transformers_['cat']\n",
    "                onehotencoder = cat_transformer.named_steps['onehot']\n",
    "\n",
    "                if hasattr(onehotencoder, 'feature_names_out_') and onehotencoder.feature_names_out_ is not None:\n",
    "                    feature_names = onehotencoder.feature_names_out_\n",
    "                    # Create feature importance dataframe\n",
    "                    if len(feature_names) == len(feature_importances):\n",
    "                        feature_importance = pd.DataFrame({\n",
    "                            'Feature': feature_names,\n",
    "                            'Importance': feature_importances\n",
    "                        })\n",
    "                        feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "                        logger.info(f\"Feature importances:\\n{feature_importance.head(20)}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not extract feature names for importances: {str(e)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not extract feature importances: {str(e)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fitting pipeline: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    # Evaluate model\n",
    "    logger.info(\"Evaluating model\")\n",
    "    results = evaluate_model(pipeline, X_test, y_test, X_train, y_train)\n",
    "\n",
    "    # Print results\n",
    "    logger.info(\"\\n=== Model Evaluation Results ===\")\n",
    "    logger.info(f\"Accuracy: {results['accuracy']:.4f}\")\n",
    "    logger.info(f\"ROC AUC: {results['roc_auc']:.4f}\")\n",
    "    logger.info(f\"PR AUC: {results['pr_auc']:.4f}\")\n",
    "    logger.info(\"\\nClassification Report:\")\n",
    "    logger.info(f\"{classification_report(y_test, pipeline.predict(X_test))}\")\n",
    "    logger.info(\"\\nConfusion Matrix:\")\n",
    "    logger.info(f\"{results['confusion_matrix']}\")\n",
    "\n",
    "    # Check for overfitting\n",
    "    if 'overfit_ratio' in results:\n",
    "        logger.info(f\"\\nTrain Accuracy: {results['train_accuracy']:.4f}\")\n",
    "        logger.info(f\"Test Accuracy: {results['accuracy']:.4f}\")\n",
    "        logger.info(f\"Overfit Ratio (train/test): {results['overfit_ratio']:.4f}\")\n",
    "\n",
    "        if results['overfit_ratio'] > 1.2:\n",
    "            logger.warning(\"Model shows signs of overfitting\")\n",
    "        elif results['overfit_ratio'] > 1.1:\n",
    "            logger.warning(\"Model shows mild signs of overfitting\")\n",
    "        else:\n",
    "            logger.info(\"Model does not show significant signs of overfitting\")\n",
    "\n",
    "    logger.info(\"=== ML Pipeline Execution Complete ===\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "a47ac2e144e348ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] 2025-05-06 14:48:41 | __main__        | INFO  | === Starting ML Pipeline Execution ===\n",
      "[✓] 2025-05-06 14:48:41 | __main__        | INFO  | Loading dataset\n",
      "[✓] 2025-05-06 14:48:41 | __main__        | INFO  | Dataset loaded successfully with shape: (45000, 14)\n",
      "[✓] 2025-05-06 14:48:41 | __main__        | INFO  | Original dataset shape: (45000, 14)\n",
      "[✓] 2025-05-06 14:48:41 | __main__        | INFO  | Original dataset columns: ['person_age', 'person_gender', 'person_education', 'person_income', 'person_emp_exp', 'person_home_ownership', 'loan_amnt', 'loan_intent', 'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length', 'credit_score', 'previous_loan_defaults_on_file', 'loan_status']\n",
      "[✓] 2025-05-06 14:48:41 | __main__        | INFO  | Original dataset data types:\n",
      "person_age                        float64\n",
      "person_gender                      object\n",
      "person_education                   object\n",
      "person_income                     float64\n",
      "person_emp_exp                      int64\n",
      "person_home_ownership              object\n",
      "loan_amnt                         float64\n",
      "loan_intent                        object\n",
      "loan_int_rate                     float64\n",
      "loan_percent_income               float64\n",
      "cb_person_cred_hist_length        float64\n",
      "credit_score                        int64\n",
      "previous_loan_defaults_on_file     object\n",
      "loan_status                         int64\n",
      "dtype: object\n",
      "[✓] 2025-05-06 14:48:41 | __main__        | INFO  | Original dataset missing values:\n",
      "person_age                        0\n",
      "person_gender                     0\n",
      "person_education                  0\n",
      "person_income                     0\n",
      "person_emp_exp                    0\n",
      "person_home_ownership             0\n",
      "loan_amnt                         0\n",
      "loan_intent                       0\n",
      "loan_int_rate                     0\n",
      "loan_percent_income               0\n",
      "cb_person_cred_hist_length        0\n",
      "credit_score                      0\n",
      "previous_loan_defaults_on_file    0\n",
      "loan_status                       0\n",
      "dtype: int64\n",
      "[✓] 2025-05-06 14:48:41 | __main__        | INFO  | Original dataset summary statistics:\n",
      "                              count          mean           std      min  \\\n",
      "person_age                  45000.0     27.764178      6.045108    20.00   \n",
      "person_income               45000.0  80319.053222  80422.498632  8000.00   \n",
      "person_emp_exp              45000.0      5.410333      6.063532     0.00   \n",
      "loan_amnt                   45000.0   9583.157556   6314.886691   500.00   \n",
      "loan_int_rate               45000.0     11.006606      2.978808     5.42   \n",
      "loan_percent_income         45000.0      0.139725      0.087212     0.00   \n",
      "cb_person_cred_hist_length  45000.0      5.867489      3.879702     2.00   \n",
      "credit_score                45000.0    632.608756     50.435865   390.00   \n",
      "loan_status                 45000.0      0.222222      0.415744     0.00   \n",
      "\n",
      "                                 25%       50%       75%         max  \n",
      "person_age                     24.00     26.00     30.00      144.00  \n",
      "person_income               47204.00  67048.00  95789.25  7200766.00  \n",
      "person_emp_exp                  1.00      4.00      8.00      125.00  \n",
      "loan_amnt                    5000.00   8000.00  12237.25    35000.00  \n",
      "loan_int_rate                   8.59     11.01     12.99       20.00  \n",
      "loan_percent_income             0.07      0.12      0.19        0.66  \n",
      "cb_person_cred_hist_length      3.00      4.00      8.00       30.00  \n",
      "credit_score                  601.00    640.00    670.00      850.00  \n",
      "loan_status                     0.00      0.00      0.00        1.00  \n",
      "[✓] 2025-05-06 14:48:41 | __main__        | INFO  | Target distribution:\n",
      "loan_status\n",
      "0    35000\n",
      "1    10000\n",
      "Name: count, dtype: int64\n",
      "[✓] 2025-05-06 14:48:41 | __main__        | INFO  | Target distribution percentage:\n",
      "loan_status\n",
      "0    77.777778\n",
      "1    22.222222\n",
      "[✓] 2025-05-06 14:48:41 | __main__        | INFO  | Splitting dataset into train and test sets\n",
      "[✓] 2025-05-06 14:48:41 | __main__        | INFO  | X shape after dropping target: (45000, 13)\n",
      "[✓] 2025-05-06 14:48:41 | __main__        | INFO  | y shape: (45000,)\n",
      "[✓] 2025-05-06 14:48:41 | __main__        | INFO  | Train-test split complete: X_train=(36000, 13), X_test=(9000, 13), y_train=(36000,), y_test=(9000,)\n",
      "[✓] 2025-05-06 14:48:41 | __main__        | INFO  | Train target distribution: {0: 28000, 1: 8000}\n",
      "[✓] 2025-05-06 14:48:41 | __main__        | INFO  | Test target distribution: {0: 7000, 1: 2000}\n",
      "[✓] 2025-05-06 14:48:41 | __main__        | INFO  | Building pipeline\n",
      "[✓] 2025-05-06 14:48:41 | __main__        | INFO  | Starting pipeline construction\n",
      "[✓] 2025-05-06 14:48:41 | __main__        | INFO  | Input DataFrame shape: (36000, 13)\n",
      "[✓] 2025-05-06 14:48:41 | __main__        | INFO  | Input columns: ['person_age', 'person_gender', 'person_education', 'person_income', 'person_emp_exp', 'person_home_ownership', 'loan_amnt', 'loan_intent', 'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length', 'credit_score', 'previous_loan_defaults_on_file']\n",
      "[✓] 2025-05-06 14:48:41 | __main__        | INFO  | Target column: loan_status\n",
      "[✓] 2025-05-06 14:48:41 | __main__        | INFO  | Loading feature store from ./references/feature_store.yaml\n",
      "[✓] 2025-05-06 14:48:41 | __main__        | INFO  | Loaded numeric columns: ['person_age', 'person_income', 'person_emp_exp', 'loan_amnt', 'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length', 'credit_score']\n",
      "[✓] 2025-05-06 14:48:41 | __main__        | INFO  | Loaded categorical columns: ['person_gender', 'person_education', 'person_home_ownership', 'loan_intent', 'previous_loan_defaults_on_file']\n",
      "[✓] 2025-05-06 14:48:41 | __main__        | INFO  | Final numeric columns: ['person_age', 'person_income', 'person_emp_exp', 'loan_amnt', 'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length', 'credit_score']\n",
      "[✓] 2025-05-06 14:48:41 | __main__        | INFO  | Final categorical columns: ['person_gender', 'person_education', 'person_home_ownership', 'loan_intent', 'previous_loan_defaults_on_file']\n",
      "[✓] 2025-05-06 14:48:41 | __main__        | INFO  | Initialized IQROutlierRemover with factor=1.5\n",
      "[✓] 2025-05-06 14:48:41 | __main__        | INFO  | Created numeric transformation pipeline\n",
      "[✓] 2025-05-06 14:48:41 | __main__        | INFO  | Created categorical transformation pipeline with drop_first=True\n",
      "[✓] 2025-05-06 14:48:41 | __main__        | INFO  | Created column transformer\n",
      "[✓] 2025-05-06 14:48:41 | __main__        | INFO  | Configured CatBoostClassifier with parameters: {'iterations': 800, 'learning_rate': 0.167, 'depth': 6, 'loss_function': 'Logloss', 'thread_count': 4, 'random_seed': 42, 'verbose': 100, 'eval_metric': 'AUC'}\n",
      "[✓] 2025-05-06 14:48:41 | __main__        | INFO  | Final pipeline created\n",
      "[✓] 2025-05-06 14:48:41 | __main__        | INFO  | Fitting pipeline\n",
      "[✓] 2025-05-06 14:48:41 | __main__        | INFO  | Initialized IQROutlierRemover with factor=1.5\n",
      "[✓] 2025-05-06 14:48:41 | __main__        | INFO  | Numeric preprocessing - input fit - DataFrame shape: (36000, 8), columns: ['person_age', 'person_income', 'person_emp_exp', 'loan_amnt', 'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length', 'credit_score']\n",
      "[✓] 2025-05-06 14:48:41 | __main__        | INFO  | Numeric preprocessing - input transform - DataFrame shape: (36000, 8), columns: ['person_age', 'person_income', 'person_emp_exp', 'loan_amnt', 'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length', 'credit_score']\n",
      "[✓] 2025-05-06 14:48:41 | __main__        | INFO  | Fitting IQROutlierRemover on data with shape (36000, 8)\n",
      "[✓] 2025-05-06 14:48:41 | __main__        | INFO  | Input columns: ['person_age', 'person_income', 'person_emp_exp', 'loan_amnt', 'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length', 'credit_score']\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Auto-detected numeric columns: ['person_age', 'person_income', 'person_emp_exp', 'loan_amnt', 'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length', 'credit_score']\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Column 'person_age': Q1=24.0000, Q3=30.0000\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Column 'person_income': Q1=47163.5000, Q3=96260.7500\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Column 'person_emp_exp': Q1=1.0000, Q3=8.0000\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Column 'loan_amnt': Q1=5000.0000, Q3=12235.0000\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Column 'loan_int_rate': Q1=8.5900, Q3=13.0225\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Column 'loan_percent_income': Q1=0.0700, Q3=0.1900\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Column 'cb_person_cred_hist_length': Q1=3.0000, Q3=8.0000\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Column 'credit_score': Q1=602.0000, Q3=670.0000\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Transforming data with IQROutlierRemover, input shape: (36000, 8)\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Column 'person_age': IQR=6.0000, lower_bound=15.0000, upper_bound=39.0000\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Column 'person_age': 0 lower outliers, 1762 upper outliers, 1762 NaN values\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Column 'person_age': Filled NaN values with median 26.0000\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Column 'person_income': IQR=49097.2500, lower_bound=-26482.3750, upper_bound=169906.6250\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Column 'person_income': 0 lower outliers, 1664 upper outliers, 1664 NaN values\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Column 'person_income': Filled NaN values with median 65718.5000\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Column 'person_emp_exp': IQR=7.0000, lower_bound=-9.5000, upper_bound=18.5000\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Column 'person_emp_exp': 0 lower outliers, 1382 upper outliers, 1382 NaN values\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Column 'person_emp_exp': Filled NaN values with median 3.0000\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Column 'loan_amnt': IQR=7235.0000, lower_bound=-5852.5000, upper_bound=23087.5000\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Column 'loan_amnt': 0 lower outliers, 1867 upper outliers, 1867 NaN values\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Column 'loan_amnt': Filled NaN values with median 8000.0000\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Column 'loan_int_rate': IQR=4.4325, lower_bound=1.9413, upper_bound=19.6712\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Column 'loan_int_rate': 0 lower outliers, 93 upper outliers, 93 NaN values\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Column 'loan_int_rate': Filled NaN values with median 11.0100\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Column 'loan_percent_income': IQR=0.1200, lower_bound=-0.1100, upper_bound=0.3700\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Column 'loan_percent_income': 0 lower outliers, 585 upper outliers, 585 NaN values\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Column 'loan_percent_income': Filled NaN values with median 0.1200\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Column 'cb_person_cred_hist_length': IQR=5.0000, lower_bound=-4.5000, upper_bound=15.5000\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Column 'cb_person_cred_hist_length': 0 lower outliers, 1092 upper outliers, 1092 NaN values\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Column 'cb_person_cred_hist_length': Filled NaN values with median 4.0000\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Column 'credit_score': IQR=68.0000, lower_bound=500.0000, upper_bound=772.0000\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Column 'credit_score': 400 lower outliers, 6 upper outliers, 406 NaN values\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Column 'credit_score': Filled NaN values with median 640.0000\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | IQROutlierRemover transformation complete, output shape: (36000, 8)\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Outlier summary: {'person_age': {'lower': 0, 'upper': 1762, 'total': 1762}, 'person_income': {'lower': 0, 'upper': 1664, 'total': 1664}, 'person_emp_exp': {'lower': 0, 'upper': 1382, 'total': 1382}, 'loan_amnt': {'lower': 0, 'upper': 1867, 'total': 1867}, 'loan_int_rate': {'lower': 0, 'upper': 93, 'total': 93}, 'loan_percent_income': {'lower': 0, 'upper': 585, 'total': 585}, 'cb_person_cred_hist_length': {'lower': 0, 'upper': 1092, 'total': 1092}, 'credit_score': {'lower': 400, 'upper': 6, 'total': 406}}\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Numeric preprocessing - output fit - array shape: (36000, 8)\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Numeric preprocessing - output transform - array shape: (36000, 8)\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Categorical preprocessing - input fit - DataFrame shape: (36000, 5), columns: ['person_gender', 'person_education', 'person_home_ownership', 'loan_intent', 'previous_loan_defaults_on_file']\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Categorical preprocessing - input transform - DataFrame shape: (36000, 5), columns: ['person_gender', 'person_education', 'person_home_ownership', 'loan_intent', 'previous_loan_defaults_on_file']\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | OneHotEncoder created the following features:\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  |   - person_gender_male\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  |   - person_education_Bachelor\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  |   - person_education_Doctorate\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  |   - person_education_High School\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  |   - person_education_Master\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  |   - person_home_ownership_OTHER\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  |   - person_home_ownership_OWN\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  |   - person_home_ownership_RENT\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  |   - loan_intent_EDUCATION\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  |   - loan_intent_HOMEIMPROVEMENT\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  |   - loan_intent_MEDICAL\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  |   - loan_intent_PERSONAL\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  |   - loan_intent_VENTURE\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  |   - previous_loan_defaults_on_file_Yes\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Categorical preprocessing - output fit - array shape: (36000, 19)\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Categorical preprocessing - output transform - array shape: (36000, 19)\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Before SMOTE fit - array shape: (36000, 27)\n",
      "[✓] 2025-05-06 14:48:42 | __main__        | INFO  | Before SMOTE transform - array shape: (36000, 27)\n",
      "[✓] 2025-05-06 14:48:43 | __main__        | INFO  | After SMOTE fit - array shape: (56000, 27)\n",
      "[✓] 2025-05-06 14:48:43 | __main__        | INFO  | After SMOTE transform - array shape: (56000, 27)\n",
      "0:\ttotal: 63ms\tremaining: 50.3s\n",
      "100:\ttotal: 4.54s\tremaining: 31.4s\n",
      "200:\ttotal: 7.91s\tremaining: 23.6s\n",
      "300:\ttotal: 9.55s\tremaining: 15.8s\n",
      "400:\ttotal: 10.9s\tremaining: 10.9s\n",
      "500:\ttotal: 12.4s\tremaining: 7.43s\n",
      "600:\ttotal: 14s\tremaining: 4.63s\n",
      "700:\ttotal: 15.5s\tremaining: 2.19s\n",
      "799:\ttotal: 17s\tremaining: 0us\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Pipeline fitting complete\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | === ONE-HOT ENCODED FEATURE NAMES ===\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  |   - person_gender_male\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  |   - person_education_Bachelor\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  |   - person_education_Doctorate\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  |   - person_education_High School\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  |   - person_education_Master\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  |   - person_home_ownership_OTHER\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  |   - person_home_ownership_OWN\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  |   - person_home_ownership_RENT\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  |   - loan_intent_EDUCATION\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  |   - loan_intent_HOMEIMPROVEMENT\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  |   - loan_intent_MEDICAL\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  |   - loan_intent_PERSONAL\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  |   - loan_intent_VENTURE\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  |   - previous_loan_defaults_on_file_Yes\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Evaluating model\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Starting model evaluation\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Test data shape: X_test=(9000, 13), y_test=(9000,)\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Train data shape: X_train=(36000, 13), y_train=(36000,)\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Generating predictions on test data\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Numeric preprocessing - input transform - DataFrame shape: (9000, 8), columns: ['person_age', 'person_income', 'person_emp_exp', 'loan_amnt', 'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length', 'credit_score']\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Transforming data with IQROutlierRemover, input shape: (9000, 8)\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'person_age': IQR=6.0000, lower_bound=15.0000, upper_bound=39.0000\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'person_age': 0 lower outliers, 426 upper outliers, 426 NaN values\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'person_age': Filled NaN values with median 26.0000\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'person_income': IQR=49097.2500, lower_bound=-26482.3750, upper_bound=169906.6250\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'person_income': 0 lower outliers, 392 upper outliers, 392 NaN values\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'person_income': Filled NaN values with median 65656.5000\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'person_emp_exp': IQR=7.0000, lower_bound=-9.5000, upper_bound=18.5000\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'person_emp_exp': 0 lower outliers, 342 upper outliers, 342 NaN values\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'person_emp_exp': Filled NaN values with median 4.0000\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'loan_amnt': IQR=7235.0000, lower_bound=-5852.5000, upper_bound=23087.5000\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'loan_amnt': 0 lower outliers, 481 upper outliers, 481 NaN values\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'loan_amnt': Filled NaN values with median 7750.0000\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'loan_int_rate': IQR=4.4325, lower_bound=1.9413, upper_bound=19.6712\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'loan_int_rate': 0 lower outliers, 27 upper outliers, 27 NaN values\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'loan_int_rate': Filled NaN values with median 11.0100\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'loan_percent_income': IQR=0.1200, lower_bound=-0.1100, upper_bound=0.3700\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'loan_percent_income': 0 lower outliers, 159 upper outliers, 159 NaN values\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'loan_percent_income': Filled NaN values with median 0.1200\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'cb_person_cred_hist_length': IQR=5.0000, lower_bound=-4.5000, upper_bound=15.5000\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'cb_person_cred_hist_length': 0 lower outliers, 274 upper outliers, 274 NaN values\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'cb_person_cred_hist_length': Filled NaN values with median 4.0000\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'credit_score': IQR=68.0000, lower_bound=500.0000, upper_bound=772.0000\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'credit_score': 106 lower outliers, 2 upper outliers, 108 NaN values\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'credit_score': Filled NaN values with median 640.0000\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | IQROutlierRemover transformation complete, output shape: (9000, 8)\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Outlier summary: {'person_age': {'lower': 0, 'upper': 426, 'total': 426}, 'person_income': {'lower': 0, 'upper': 392, 'total': 392}, 'person_emp_exp': {'lower': 0, 'upper': 342, 'total': 342}, 'loan_amnt': {'lower': 0, 'upper': 481, 'total': 481}, 'loan_int_rate': {'lower': 0, 'upper': 27, 'total': 27}, 'loan_percent_income': {'lower': 0, 'upper': 159, 'total': 159}, 'cb_person_cred_hist_length': {'lower': 0, 'upper': 274, 'total': 274}, 'credit_score': {'lower': 106, 'upper': 2, 'total': 108}}\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Numeric preprocessing - output transform - array shape: (9000, 8)\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Categorical preprocessing - input transform - DataFrame shape: (9000, 5), columns: ['person_gender', 'person_education', 'person_home_ownership', 'loan_intent', 'previous_loan_defaults_on_file']\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Categorical preprocessing - output transform - array shape: (9000, 19)\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Before SMOTE transform - array shape: (9000, 27)\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | After SMOTE transform - array shape: (9000, 27)\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Numeric preprocessing - input transform - DataFrame shape: (9000, 8), columns: ['person_age', 'person_income', 'person_emp_exp', 'loan_amnt', 'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length', 'credit_score']\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Transforming data with IQROutlierRemover, input shape: (9000, 8)\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'person_age': IQR=6.0000, lower_bound=15.0000, upper_bound=39.0000\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'person_age': 0 lower outliers, 426 upper outliers, 426 NaN values\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'person_age': Filled NaN values with median 26.0000\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'person_income': IQR=49097.2500, lower_bound=-26482.3750, upper_bound=169906.6250\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'person_income': 0 lower outliers, 392 upper outliers, 392 NaN values\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'person_income': Filled NaN values with median 65656.5000\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'person_emp_exp': IQR=7.0000, lower_bound=-9.5000, upper_bound=18.5000\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'person_emp_exp': 0 lower outliers, 342 upper outliers, 342 NaN values\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'person_emp_exp': Filled NaN values with median 4.0000\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'loan_amnt': IQR=7235.0000, lower_bound=-5852.5000, upper_bound=23087.5000\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'loan_amnt': 0 lower outliers, 481 upper outliers, 481 NaN values\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'loan_amnt': Filled NaN values with median 7750.0000\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'loan_int_rate': IQR=4.4325, lower_bound=1.9413, upper_bound=19.6712\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'loan_int_rate': 0 lower outliers, 27 upper outliers, 27 NaN values\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'loan_int_rate': Filled NaN values with median 11.0100\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'loan_percent_income': IQR=0.1200, lower_bound=-0.1100, upper_bound=0.3700\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'loan_percent_income': 0 lower outliers, 159 upper outliers, 159 NaN values\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'loan_percent_income': Filled NaN values with median 0.1200\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'cb_person_cred_hist_length': IQR=5.0000, lower_bound=-4.5000, upper_bound=15.5000\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'cb_person_cred_hist_length': 0 lower outliers, 274 upper outliers, 274 NaN values\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'cb_person_cred_hist_length': Filled NaN values with median 4.0000\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'credit_score': IQR=68.0000, lower_bound=500.0000, upper_bound=772.0000\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'credit_score': 106 lower outliers, 2 upper outliers, 108 NaN values\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Column 'credit_score': Filled NaN values with median 640.0000\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | IQROutlierRemover transformation complete, output shape: (9000, 8)\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Outlier summary: {'person_age': {'lower': 0, 'upper': 426, 'total': 426}, 'person_income': {'lower': 0, 'upper': 392, 'total': 392}, 'person_emp_exp': {'lower': 0, 'upper': 342, 'total': 342}, 'loan_amnt': {'lower': 0, 'upper': 481, 'total': 481}, 'loan_int_rate': {'lower': 0, 'upper': 27, 'total': 27}, 'loan_percent_income': {'lower': 0, 'upper': 159, 'total': 159}, 'cb_person_cred_hist_length': {'lower': 0, 'upper': 274, 'total': 274}, 'credit_score': {'lower': 106, 'upper': 2, 'total': 108}}\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Numeric preprocessing - output transform - array shape: (9000, 8)\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Categorical preprocessing - input transform - DataFrame shape: (9000, 5), columns: ['person_gender', 'person_education', 'person_home_ownership', 'loan_intent', 'previous_loan_defaults_on_file']\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Categorical preprocessing - output transform - array shape: (9000, 19)\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | Before SMOTE transform - array shape: (9000, 27)\n",
      "[✓] 2025-05-06 14:49:01 | __main__        | INFO  | After SMOTE transform - array shape: (9000, 27)\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Actual class distribution: {0: 7000, 1: 2000}\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Predicted class distribution: {0: 7084, 1: 1916}\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Test accuracy: 0.9307\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Classification report:\n",
      "              precision    recall  f1-score      support\n",
      "0              0.950028  0.961429  0.955694  7000.000000\n",
      "1              0.859081  0.823000  0.840654  2000.000000\n",
      "accuracy       0.930667  0.930667  0.930667     0.930667\n",
      "macro avg      0.904555  0.892214  0.898174  9000.000000\n",
      "weighted avg   0.929818  0.930667  0.930130  9000.000000\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Confusion matrix:\n",
      "[[6730  270]\n",
      " [ 354 1646]]\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | ROC AUC: 0.9764\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | PR AUC: 0.9345\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Generating predictions on training data to check for overfitting\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Numeric preprocessing - input transform - DataFrame shape: (36000, 8), columns: ['person_age', 'person_income', 'person_emp_exp', 'loan_amnt', 'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length', 'credit_score']\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Transforming data with IQROutlierRemover, input shape: (36000, 8)\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'person_age': IQR=6.0000, lower_bound=15.0000, upper_bound=39.0000\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'person_age': 0 lower outliers, 1762 upper outliers, 1762 NaN values\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'person_age': Filled NaN values with median 26.0000\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'person_income': IQR=49097.2500, lower_bound=-26482.3750, upper_bound=169906.6250\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'person_income': 0 lower outliers, 1664 upper outliers, 1664 NaN values\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'person_income': Filled NaN values with median 65718.5000\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'person_emp_exp': IQR=7.0000, lower_bound=-9.5000, upper_bound=18.5000\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'person_emp_exp': 0 lower outliers, 1382 upper outliers, 1382 NaN values\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'person_emp_exp': Filled NaN values with median 3.0000\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'loan_amnt': IQR=7235.0000, lower_bound=-5852.5000, upper_bound=23087.5000\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'loan_amnt': 0 lower outliers, 1867 upper outliers, 1867 NaN values\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'loan_amnt': Filled NaN values with median 8000.0000\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'loan_int_rate': IQR=4.4325, lower_bound=1.9413, upper_bound=19.6712\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'loan_int_rate': 0 lower outliers, 93 upper outliers, 93 NaN values\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'loan_int_rate': Filled NaN values with median 11.0100\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'loan_percent_income': IQR=0.1200, lower_bound=-0.1100, upper_bound=0.3700\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'loan_percent_income': 0 lower outliers, 585 upper outliers, 585 NaN values\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'loan_percent_income': Filled NaN values with median 0.1200\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'cb_person_cred_hist_length': IQR=5.0000, lower_bound=-4.5000, upper_bound=15.5000\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'cb_person_cred_hist_length': 0 lower outliers, 1092 upper outliers, 1092 NaN values\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'cb_person_cred_hist_length': Filled NaN values with median 4.0000\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'credit_score': IQR=68.0000, lower_bound=500.0000, upper_bound=772.0000\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'credit_score': 400 lower outliers, 6 upper outliers, 406 NaN values\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'credit_score': Filled NaN values with median 640.0000\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | IQROutlierRemover transformation complete, output shape: (36000, 8)\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Outlier summary: {'person_age': {'lower': 0, 'upper': 1762, 'total': 1762}, 'person_income': {'lower': 0, 'upper': 1664, 'total': 1664}, 'person_emp_exp': {'lower': 0, 'upper': 1382, 'total': 1382}, 'loan_amnt': {'lower': 0, 'upper': 1867, 'total': 1867}, 'loan_int_rate': {'lower': 0, 'upper': 93, 'total': 93}, 'loan_percent_income': {'lower': 0, 'upper': 585, 'total': 585}, 'cb_person_cred_hist_length': {'lower': 0, 'upper': 1092, 'total': 1092}, 'credit_score': {'lower': 400, 'upper': 6, 'total': 406}}\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Numeric preprocessing - output transform - array shape: (36000, 8)\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Categorical preprocessing - input transform - DataFrame shape: (36000, 5), columns: ['person_gender', 'person_education', 'person_home_ownership', 'loan_intent', 'previous_loan_defaults_on_file']\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Categorical preprocessing - output transform - array shape: (36000, 19)\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Before SMOTE transform - array shape: (36000, 27)\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | After SMOTE transform - array shape: (36000, 27)\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Train accuracy: 0.9792\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Overfit ratio (train/test): 1.0521\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Model evaluation complete\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | \n",
      "=== Model Evaluation Results ===\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Accuracy: 0.9307\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | ROC AUC: 0.9764\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | PR AUC: 0.9345\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | \n",
      "Classification Report:\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Numeric preprocessing - input transform - DataFrame shape: (9000, 8), columns: ['person_age', 'person_income', 'person_emp_exp', 'loan_amnt', 'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length', 'credit_score']\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Transforming data with IQROutlierRemover, input shape: (9000, 8)\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'person_age': IQR=6.0000, lower_bound=15.0000, upper_bound=39.0000\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'person_age': 0 lower outliers, 426 upper outliers, 426 NaN values\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'person_age': Filled NaN values with median 26.0000\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'person_income': IQR=49097.2500, lower_bound=-26482.3750, upper_bound=169906.6250\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'person_income': 0 lower outliers, 392 upper outliers, 392 NaN values\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'person_income': Filled NaN values with median 65656.5000\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'person_emp_exp': IQR=7.0000, lower_bound=-9.5000, upper_bound=18.5000\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'person_emp_exp': 0 lower outliers, 342 upper outliers, 342 NaN values\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'person_emp_exp': Filled NaN values with median 4.0000\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'loan_amnt': IQR=7235.0000, lower_bound=-5852.5000, upper_bound=23087.5000\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'loan_amnt': 0 lower outliers, 481 upper outliers, 481 NaN values\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'loan_amnt': Filled NaN values with median 7750.0000\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'loan_int_rate': IQR=4.4325, lower_bound=1.9413, upper_bound=19.6712\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'loan_int_rate': 0 lower outliers, 27 upper outliers, 27 NaN values\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'loan_int_rate': Filled NaN values with median 11.0100\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'loan_percent_income': IQR=0.1200, lower_bound=-0.1100, upper_bound=0.3700\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'loan_percent_income': 0 lower outliers, 159 upper outliers, 159 NaN values\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'loan_percent_income': Filled NaN values with median 0.1200\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'cb_person_cred_hist_length': IQR=5.0000, lower_bound=-4.5000, upper_bound=15.5000\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'cb_person_cred_hist_length': 0 lower outliers, 274 upper outliers, 274 NaN values\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'cb_person_cred_hist_length': Filled NaN values with median 4.0000\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'credit_score': IQR=68.0000, lower_bound=500.0000, upper_bound=772.0000\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'credit_score': 106 lower outliers, 2 upper outliers, 108 NaN values\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Column 'credit_score': Filled NaN values with median 640.0000\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | IQROutlierRemover transformation complete, output shape: (9000, 8)\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Outlier summary: {'person_age': {'lower': 0, 'upper': 426, 'total': 426}, 'person_income': {'lower': 0, 'upper': 392, 'total': 392}, 'person_emp_exp': {'lower': 0, 'upper': 342, 'total': 342}, 'loan_amnt': {'lower': 0, 'upper': 481, 'total': 481}, 'loan_int_rate': {'lower': 0, 'upper': 27, 'total': 27}, 'loan_percent_income': {'lower': 0, 'upper': 159, 'total': 159}, 'cb_person_cred_hist_length': {'lower': 0, 'upper': 274, 'total': 274}, 'credit_score': {'lower': 106, 'upper': 2, 'total': 108}}\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Numeric preprocessing - output transform - array shape: (9000, 8)\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Categorical preprocessing - input transform - DataFrame shape: (9000, 5), columns: ['person_gender', 'person_education', 'person_home_ownership', 'loan_intent', 'previous_loan_defaults_on_file']\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Categorical preprocessing - output transform - array shape: (9000, 19)\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Before SMOTE transform - array shape: (9000, 27)\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | After SMOTE transform - array shape: (9000, 27)\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  |               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96      7000\n",
      "           1       0.86      0.82      0.84      2000\n",
      "\n",
      "    accuracy                           0.93      9000\n",
      "   macro avg       0.90      0.89      0.90      9000\n",
      "weighted avg       0.93      0.93      0.93      9000\n",
      "\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | \n",
      "Confusion Matrix:\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | [[6730  270]\n",
      " [ 354 1646]]\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | \n",
      "Train Accuracy: 0.9792\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Test Accuracy: 0.9307\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Overfit Ratio (train/test): 1.0521\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | Model does not show significant signs of overfitting\n",
      "[✓] 2025-05-06 14:49:02 | __main__        | INFO  | === ML Pipeline Execution Complete ===\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T19:49:50.464505Z",
     "start_time": "2025-05-06T19:49:50.369155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "from catboost import CatBoostClassifier\n",
    "from src.data.data_transformation import PreprocessingPipeline\n",
    "\n",
    "def main():\n",
    "    # Load the preprocessing pipeline\n",
    "    pipeline_path = \"./models/preprocessor/preprocessing_pipeline.pkl\"\n",
    "    try:\n",
    "        preprocessing_pipeline = joblib.load(pipeline_path)\n",
    "        print(\"Pipeline loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading pipeline: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    # Create test data\n",
    "    data = {\n",
    "        \"person_age\": [20],\n",
    "        \"person_gender\": [\"male\"],\n",
    "        \"person_education\": [\"Bachelor\"],\n",
    "        \"person_income\": [10000],\n",
    "        \"person_emp_exp\": [2],\n",
    "        \"person_home_ownership\": [\"OWN\"],\n",
    "        \"loan_amnt\": [5000],\n",
    "        \"loan_intent\": [\"EDUCATION\"],\n",
    "        \"loan_int_rate\": [4.5],\n",
    "        \"loan_percent_income\": [0.04],\n",
    "        \"cb_person_cred_hist_length\": [3],\n",
    "        \"credit_score\": [498],\n",
    "        \"previous_loan_defaults_on_file\": [\"Yes\"]\n",
    "    }\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Add random target column (not needed for prediction but helps with pipeline expectations)\n",
    "\n",
    "    try:\n",
    "        # Verify required columns exist\n",
    "        required_columns = preprocessing_pipeline.numeric_cols + preprocessing_pipeline.categorical_cols\n",
    "        missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"Missing columns in test data: {missing_cols}\")\n",
    "            return\n",
    "\n",
    "        # Transform data\n",
    "        X_transformed = preprocessing_pipeline.transform(df)\n",
    "\n",
    "        # Convert to DataFrame with feature names\n",
    "        X_transformed_df = pd.DataFrame(X_transformed, columns=preprocessing_pipeline.get_feature_names_out())\n",
    "\n",
    "        # Display transformation info\n",
    "        print(\"\\nTransformed Feature Array Shape:\", X_transformed_df.shape)\n",
    "        print(\"\\nTransformed Feature Names:\")\n",
    "        print(X_transformed_df.columns.tolist())\n",
    "        print(\"\\nFirst Transformed Sample:\")\n",
    "        print(X_transformed_df.iloc[0])\n",
    "\n",
    "        # Load trained CatBoost model\n",
    "        model_path = \"./models/model/model.cbm\"\n",
    "        model = CatBoostClassifier()\n",
    "        model.load_model(model_path)\n",
    "        print(\"\\nModel loaded successfully\")\n",
    "\n",
    "        # Predict using the model\n",
    "        prediction = model.predict(X_transformed_df)\n",
    "        print(\"\\nModel Prediction:\", prediction[0])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "bcbac1dc25cd5916",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline loaded successfully\n",
      "\n",
      "Transformed Feature Array Shape: (1, 22)\n",
      "\n",
      "Transformed Feature Names:\n",
      "['person_age', 'person_income', 'person_emp_exp', 'loan_amnt', 'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length', 'credit_score', 'person_gender_male', 'person_education_Bachelor', 'person_education_Doctorate', 'person_education_High School', 'person_education_Master', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'previous_loan_defaults_on_file_Yes']\n",
      "\n",
      "First Transformed Sample:\n",
      "person_age                           -2.472941\n",
      "person_income                        -3.747876\n",
      "person_emp_exp                       -0.372963\n",
      "loan_amnt                            -0.661598\n",
      "loan_int_rate                         4.500000\n",
      "loan_percent_income                  -1.411752\n",
      "cb_person_cred_hist_length           -0.797265\n",
      "credit_score                          0.045593\n",
      "person_gender_male                    1.000000\n",
      "person_education_Bachelor             1.000000\n",
      "person_education_Doctorate            0.000000\n",
      "person_education_High School          0.000000\n",
      "person_education_Master               0.000000\n",
      "person_home_ownership_OTHER           0.000000\n",
      "person_home_ownership_OWN             1.000000\n",
      "person_home_ownership_RENT            0.000000\n",
      "loan_intent_EDUCATION                 1.000000\n",
      "loan_intent_HOMEIMPROVEMENT           0.000000\n",
      "loan_intent_MEDICAL                   0.000000\n",
      "loan_intent_PERSONAL                  0.000000\n",
      "loan_intent_VENTURE                   0.000000\n",
      "previous_loan_defaults_on_file_Yes    1.000000\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "Model loaded successfully\n",
      "\n",
      "Model Prediction: 0\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import joblib\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import os\n",
    "\n",
    "# Create sample data for testing\n",
    "data = {\n",
    "    \"person_age\": [32.0, 24.0],\n",
    "    \"person_gender\": [\"male\", \"male\"],\n",
    "    \"person_education\": [\"Associate\", \"Associate\"],\n",
    "    \"person_income\": [96865.0, 56838.0],\n",
    "    \"person_emp_exp\": [10, 6],\n",
    "    \"person_home_ownership\": [\"MORTGAGE\", \"RENT\"],\n",
    "    \"loan_amnt\": [7500.0, 9000.0],\n",
    "    \"loan_intent\": [\"EDUCATION\", \"EDUCATION\"],\n",
    "    \"loan_int_rate\": [6.04, 11.49],\n",
    "    \"loan_percent_income\": [0.08, 0.16],\n",
    "    \"cb_person_cred_hist_length\": [10.0, 4.0],\n",
    "    \"credit_score\": [601, 647],\n",
    "    \"previous_loan_defaults_on_file\": [\"No\", \"Yes\"],\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Randomly generate target column for demonstration purposes\n",
    "df[\"loan_status\"] = np.random.randint(0, 2, size=len(df))\n",
    "\n",
    "# Define numeric and categorical columns\n",
    "\n",
    "numeric_cols = [\n",
    "    \"person_age\",\n",
    "    \"person_income\",\n",
    "    \"person_emp_exp\",\n",
    "    \"loan_amnt\",\n",
    "    \"loan_int_rate\",\n",
    "    \"loan_percent_income\",\n",
    "    \"cb_person_cred_hist_length\",\n",
    "    \"credit_score\"\n",
    "]\n",
    "\n",
    "# 1. Create the preprocessing pipeline (make sure this is a class instance, not an array)\n",
    "from your_module_path import PreprocessingPipeline  # Import your actual class\n",
    "# Or use this simplified version if needed:\n",
    "'''\n",
    "class PreprocessingPipeline:\n",
    "    def __init__(self, numeric_cols, categorical_cols):\n",
    "        self.numeric_cols = numeric_cols\n",
    "        self.categorical_cols = categorical_cols\n",
    "        self.feature_names_out_ = None\n",
    "        self.transformers = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Your fit logic here\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Your transform logic here\n",
    "        return X_transformed\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array(self.feature_names_out_)\n",
    "'''\n",
    "\n",
    "# Create the pipeline instance\n",
    "preprocessing_pipeline = PreprocessingPipeline(numeric_cols, categorical_cols)\n",
    "\n",
    "# 2. Fit the preprocessing pipeline on your data\n",
    "preprocessing_pipeline = preprocessing_pipeline.fit(df.drop(columns=['loan_status']))\n",
    "# NOTE: Even though it's common to chain methods like this, make sure to assign it back to the same variable\n",
    "\n",
    "# 3. Now use the pipeline to transform your data\n",
    "X_transformed = preprocessing_pipeline.transform(df.drop(columns=['loan_status']))\n",
    "# DO NOT do: preprocessing_pipeline = preprocessing_pipeline.get_feature_names_out()\n",
    "\n",
    "# 4. If you need feature names, store them in a separate variable\n",
    "feature_names = preprocessing_pipeline.get_feature_names_out()\n",
    "\n",
    "# 5. Output transformed result\n",
    "print(\"Transformed Feature Array:\")\n",
    "print(X_transformed.shape)\n",
    "print(feature_names)"
   ],
   "id": "8c44980c09fd7147"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "from demo_src.data.data_transformation import PreprocessingPipeline\n",
    "\n",
    "# Load the pipeline\n",
    "pipeline_path = os.path.join(\"demo_artifacts\", \"preprocessing_pipeline.pkl\")\n",
    "preprocessing_pipeline = joblib.load(pipeline_path)\n",
    "\n",
    "# Create test data\n",
    "data = {\n",
    "    \"person_age\": [32.0, 24.0],\n",
    "    \"person_gender\": [\"male\", \"male\"],\n",
    "    \"person_education\": [\"Associate\", \"Associate\"],\n",
    "    \"person_income\": [96865.0, 56838.0],\n",
    "    \"person_emp_exp\": [10, 6],\n",
    "    \"person_home_ownership\": [\"MORTGAGE\", \"RENT\"],\n",
    "    \"loan_amnt\": [7500.0, 9000.0],\n",
    "    \"loan_intent\": [\"EDUCATION\", \"EDUCATION\"],\n",
    "    \"loan_int_rate\": [6.04, 11.49],\n",
    "    \"loan_percent_income\": [0.08, 0.16],\n",
    "    \"cb_person_cred_hist_length\": [10.0, 4.0],\n",
    "    \"credit_score\": [601, 647],\n",
    "    \"previous_loan_defaults_on_file\": [\"No\", \"Yes\"]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Randomly generate target column for demonstration purposes\n",
    "df[\"loan_status\"] = np.random.randint(0, 2, size=len(df))\n",
    "\n",
    "# Transform data\n",
    "X_transformed = preprocessing_pipeline.transform(df.drop(columns=[\"loan_status\"]))\n",
    "\n",
    "# Output transformed result\n",
    "print(\"Transformed Feature Array:\")\n",
    "print(X_transformed)\n"
   ],
   "id": "8a1a425758440bb9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
